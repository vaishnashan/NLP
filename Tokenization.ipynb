{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8aiTYjHaAj7OR8u3rP9HN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnashan/NLP/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nmns8SPd2p3",
        "outputId": "e27b8fea-3015-43c2-8021-6cf77ed392c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Download required NLTK packages\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize Stemmer and Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input paragraph\n",
        "paragraph = \"\"\"In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[9]\n",
        "\n",
        "In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing.[14][15] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[16] or protect patient privacy.[17]\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "kjSKsbtheGNe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the paragraph into sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Preprocessing: Cleaning, Tokenization, Removing Stopwords, Lemmatization\n",
        "corpus = []\n",
        "for sentence in sentences:\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "\n",
        "    # Remove stopwords and apply lemmatization\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
        "\n",
        "    # Rejoin words into a single string\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n",
        "# Output the cleaned corpus\n",
        "print(\"Cleaned Corpus:\")\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHSt9Q4eeWHK",
        "outputId": "66de81a0-998a-4ecb-89f8-4c17ef7929e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Corpus:\n",
            "['word n gram model time best statistical algorithm outperformed multi layer perceptron single hidden layer context length several word trained million word cpu cluster language modelling yoshua bengio co author', 'tom mikolov phd student brno university technology co author applied simple recurrent neural network single hidden layer language modelling following year went develop word vec', 'representation learning deep neural network style featuring many hidden layer machine learning method became widespread natural language processing', 'popularity due partly flurry result showing technique achieve state art result many natural language task e g language modeling parsing', 'increasingly important medicine healthcare nlp help analyze note text electronic health record would otherwise inaccessible study seeking improve care protect patient privacy', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform CountVectorization\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus)\n",
        "\n",
        "# Display the vocabulary and feature matrix\n",
        "print(\"Vocabulary:\", cv.vocabulary_)\n",
        "print(\"Feature Array for First Sentence:\", x[0].toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9msWUhoNekAY",
        "outputId": "7b57d4c6-0441-49fb-acec-73d3e2476bb6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'word': 86, 'gram': 22, 'model': 41, 'time': 79, 'best': 8, 'statistical': 71, 'algorithm': 1, 'outperformed': 51, 'multi': 44, 'layer': 32, 'perceptron': 55, 'single': 69, 'hidden': 26, 'context': 13, 'length': 34, 'several': 66, 'trained': 81, 'million': 40, 'cpu': 14, 'cluster': 11, 'language': 31, 'modelling': 43, 'yoshua': 89, 'bengio': 7, 'co': 12, 'author': 5, 'tom': 80, 'mikolov': 39, 'phd': 56, 'student': 72, 'brno': 9, 'university': 82, 'technology': 77, 'applied': 3, 'simple': 68, 'recurrent': 62, 'neural': 47, 'network': 46, 'following': 21, 'year': 88, 'went': 84, 'develop': 16, 'vec': 83, 'representation': 63, 'learning': 33, 'deep': 15, 'style': 74, 'featuring': 19, 'many': 36, 'machine': 35, 'method': 38, 'became': 6, 'widespread': 85, 'natural': 45, 'processing': 59, 'popularity': 57, 'due': 17, 'partly': 53, 'flurry': 20, 'result': 64, 'showing': 67, 'technique': 76, 'achieve': 0, 'state': 70, 'art': 4, 'task': 75, 'modeling': 42, 'parsing': 52, 'increasingly': 30, 'important': 27, 'medicine': 37, 'healthcare': 24, 'nlp': 48, 'help': 25, 'analyze': 2, 'note': 49, 'text': 78, 'electronic': 18, 'health': 23, 'record': 61, 'would': 87, 'otherwise': 50, 'inaccessible': 29, 'study': 73, 'seeking': 65, 'improve': 28, 'care': 10, 'protect': 60, 'patient': 54, 'privacy': 58}\n",
            "Feature Array for First Sentence: [[0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 2 0 1 0\n",
            "  0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            "  0 0 0 0 0 0 0 1 0 1 0 0 0 0 3 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of stemming and lemmatization\n",
        "sample_word = \"history\"\n",
        "print(\"Stemmed Word:\", stemmer.stem(sample_word))\n",
        "\n",
        "sample_word_lemma = \"history\"\n",
        "print(\"Lemmatized Word:\", lemmatizer.lemmatize(sample_word_lemma))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGIFcNd1eB4p",
        "outputId": "f1419fff-e6f5-4f7a-d054-abfcec23d9df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Word: histori\n",
            "Lemmatized Word: history\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10KPMZeCe70U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}