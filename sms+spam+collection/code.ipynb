{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download necessary resources from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"C:/Users/DELL/Desktop/sms+spam+collection/SMSSpamCollection\"  # Path to your uploaded file\n",
    "data = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply preprocessing to the 'message' column\n",
    "data['cleaned_message'] = data['message'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point , crazi .. avail bugi n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar ... joke wif u oni ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor ... u c alreadi say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah n't think goe usf , live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  go jurong point , crazi .. avail bugi n great ...  \n",
       "1                      ok lar ... joke wif u oni ...  \n",
       "2  free entri 2 wkli comp win fa cup final tkt 21...  \n",
       "3        u dun say earli hor ... u c alreadi say ...  \n",
       "4         nah n't think goe usf , live around though  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BOW) Features:\n",
      "   00  000  000pe  008704050406  0089  0121  01223585236  01223585334  \\\n",
      "0   0    0      0             0     0     0            0            0   \n",
      "1   0    0      0             0     0     0            0            0   \n",
      "2   0    0      0             0     0     0            0            0   \n",
      "3   0    0      0             0     0     0            0            0   \n",
      "4   0    0      0             0     0     0            0            0   \n",
      "\n",
      "   0125698789  02  ...  zhong  zindgi  zoe  zogtoriu  zoom  zouk  zyada  èn  \\\n",
      "0           0   0  ...      0       0    0         0     0     0      0   0   \n",
      "1           0   0  ...      0       0    0         0     0     0      0   0   \n",
      "2           0   0  ...      0       0    0         0     0     0      0   0   \n",
      "3           0   0  ...      0       0    0         0     0     0      0   0   \n",
      "4           0   0  ...      0       0    0         0     0     0      0   0   \n",
      "\n",
      "   ú1  〨ud  \n",
      "0   0    0  \n",
      "1   0    0  \n",
      "2   0    0  \n",
      "3   0    0  \n",
      "4   0    0  \n",
      "\n",
      "[5 rows x 7394 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Bag of Words (BOW)\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(data['cleaned_message'])\n",
    "\n",
    "# You can convert these sparse matrices to arrays or DataFrames if needed\n",
    "X_bow_df = pd.DataFrame(X_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the resulting feature matrices\n",
    "print(\"Bag of Words (BOW) Features:\")\n",
    "print(X_bow_df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Features:\n",
      "    00  000  000pe  008704050406  0089  0121  01223585236  01223585334  \\\n",
      "0  0.0  0.0    0.0           0.0   0.0   0.0          0.0          0.0   \n",
      "1  0.0  0.0    0.0           0.0   0.0   0.0          0.0          0.0   \n",
      "2  0.0  0.0    0.0           0.0   0.0   0.0          0.0          0.0   \n",
      "3  0.0  0.0    0.0           0.0   0.0   0.0          0.0          0.0   \n",
      "4  0.0  0.0    0.0           0.0   0.0   0.0          0.0          0.0   \n",
      "\n",
      "   0125698789   02  ...  zhong  zindgi  zoe  zogtoriu  zoom  zouk  zyada   èn  \\\n",
      "0         0.0  0.0  ...    0.0     0.0  0.0       0.0   0.0   0.0    0.0  0.0   \n",
      "1         0.0  0.0  ...    0.0     0.0  0.0       0.0   0.0   0.0    0.0  0.0   \n",
      "2         0.0  0.0  ...    0.0     0.0  0.0       0.0   0.0   0.0    0.0  0.0   \n",
      "3         0.0  0.0  ...    0.0     0.0  0.0       0.0   0.0   0.0    0.0  0.0   \n",
      "4         0.0  0.0  ...    0.0     0.0  0.0       0.0   0.0   0.0    0.0  0.0   \n",
      "\n",
      "    ú1  〨ud  \n",
      "0  0.0  0.0  \n",
      "1  0.0  0.0  \n",
      "2  0.0  0.0  \n",
      "3  0.0  0.0  \n",
      "4  0.0  0.0  \n",
      "\n",
      "[5 rows x 7394 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['cleaned_message'])\n",
    "\n",
    "# You can convert these sparse matrices to arrays or DataFrames if needed\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the resulting feature matrices\n",
    "print(\"\\nTF-IDF Features:\")\n",
    "print(X_tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-grams Features:\n",
      "   00  00 easter  00 easter prize  00 per  00 sub  00 sub 16  000  000 bonu  \\\n",
      "0   0          0                0       0       0          0    0         0   \n",
      "1   0          0                0       0       0          0    0         0   \n",
      "2   0          0                0       0       0          0    0         0   \n",
      "3   0          0                0       0       0          0    0         0   \n",
      "4   0          0                0       0       0          0    0         0   \n",
      "\n",
      "   000 bonu caller  000 cash  ...  zouk nichol paris  zyada  zyada kisi  \\\n",
      "0                0         0  ...                  0      0           0   \n",
      "1                0         0  ...                  0      0           0   \n",
      "2                0         0  ...                  0      0           0   \n",
      "3                0         0  ...                  0      0           0   \n",
      "4                0         0  ...                  0      0           0   \n",
      "\n",
      "   zyada kisi ko  èn  ú1  ú1 20  ú1 20 poboxox36504w45wq  〨ud  〨ud even  \n",
      "0              0   0   0      0                        0    0         0  \n",
      "1              0   0   0      0                        0    0         0  \n",
      "2              0   0   0      0                        0    0         0  \n",
      "3              0   0   0      0                        0    0         0  \n",
      "4              0   0   0      0                        0    0         0  \n",
      "\n",
      "[5 rows x 70704 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3. N-grams (example: using bi-grams and tri-grams)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X_ngram = ngram_vectorizer.fit_transform(data['cleaned_message'])\n",
    "\n",
    "# You can convert these sparse matrices to arrays or DataFrames if needed\n",
    "X_ngram_df = pd.DataFrame(X_ngram.toarray(), columns=ngram_vectorizer.get_feature_names_out())\n",
    "                               \n",
    "# Display the first few rows of the resulting feature matrices\n",
    "print(\"\\nN-grams Features:\")\n",
    "print(X_ngram_df.head())\n",
    "\n",
    "          \n",
    "\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
