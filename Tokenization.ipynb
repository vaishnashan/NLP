{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdR8JC4R4FKxdOZ60yqyLH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnashan/NLP/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Required NLTK Packages\n",
        "# stopwords: Contains a list of common stopwords for many languages.\n",
        "# punkt: For sentence and word tokenization.\n",
        "# wordnet: A lexical database used for lemmatization.\n"
      ],
      "metadata": {
        "id": "C9k7Oz1JiiH9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nmns8SPd2p3",
        "outputId": "b4a00786-8014-43e8-9781-a5a0cd2e7a5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Download required NLTK packages\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize Stemmer and Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input paragraph\n",
        "paragraph = \"\"\"In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[9]\n",
        "\n",
        "In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing.[14][15] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[16] or protect patient privacy.[17]\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "kjSKsbtheGNe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the paragraph into sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Preprocessing: Cleaning, Tokenization, Removing Stopwords, Lemmatization\n",
        "corpus = []\n",
        "for sentence in sentences:\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentence)  # Removes numbers, punctuation, and special characters, leaving only alphabets.\n",
        "    review = review.lower()                     # Simplifies word comparison (e.g., \"The\" = \"the\").\n",
        "    review = review.split()                        #okenizes the sentence into a list of words.\n",
        "\n",
        "    # Remove stopwords and apply lemmatization\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]  #Filters out common stopwords and Converts words to their base form (e.g., \"running\" → \"run\").\n",
        "\n",
        "    # Rejoin words into a single string\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)                #joins the cleaned words back into a processed sentence.\n",
        "\n",
        "# Output the cleaned corpus\n",
        "print(\"Cleaned Corpus:\")\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHSt9Q4eeWHK",
        "outputId": "33d7b873-05bb-4d69-9981-6d7bb0e89633"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Corpus:\n",
            "['word n gram model time best statistical algorithm outperformed multi layer perceptron single hidden layer context length several word trained million word cpu cluster language modelling yoshua bengio co author', 'tom mikolov phd student brno university technology co author applied simple recurrent neural network single hidden layer language modelling following year went develop word vec', 'representation learning deep neural network style featuring many hidden layer machine learning method became widespread natural language processing', 'popularity due partly flurry result showing technique achieve state art result many natural language task e g language modeling parsing', 'increasingly important medicine healthcare nlp help analyze note text electronic health record would otherwise inaccessible study seeking improve care protect patient privacy', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer: From sklearn, transforms text data into numerical features for modeling.\n",
        "# Converts the cleaned text (corpus) into a numerical representation.\n",
        "# The representation is based on the frequency of words (bag-of-words model).------BoW"
      ],
      "metadata": {
        "id": "AYwdHgp_iRT_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform CountVectorization\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus)   #Learns the vocabulary from the corpus and creates a sparse matrix of word frequencies.\n"
      ],
      "metadata": {
        "id": "9msWUhoNekAY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APNh9yNbj5mJ",
        "outputId": "3e9574e2-c069-441e-c030-d82d58b1917e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6x90 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 106 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary:\", cv.vocabulary_)    #Displays the mapping of words to their index positions in the feature matrix."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dDuqEgXj5Hs",
        "outputId": "63a6c6b8-c66f-44fc-b175-b942ec325cca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'word': 86, 'gram': 22, 'model': 41, 'time': 79, 'best': 8, 'statistical': 71, 'algorithm': 1, 'outperformed': 51, 'multi': 44, 'layer': 32, 'perceptron': 55, 'single': 69, 'hidden': 26, 'context': 13, 'length': 34, 'several': 66, 'trained': 81, 'million': 40, 'cpu': 14, 'cluster': 11, 'language': 31, 'modelling': 43, 'yoshua': 89, 'bengio': 7, 'co': 12, 'author': 5, 'tom': 80, 'mikolov': 39, 'phd': 56, 'student': 72, 'brno': 9, 'university': 82, 'technology': 77, 'applied': 3, 'simple': 68, 'recurrent': 62, 'neural': 47, 'network': 46, 'following': 21, 'year': 88, 'went': 84, 'develop': 16, 'vec': 83, 'representation': 63, 'learning': 33, 'deep': 15, 'style': 74, 'featuring': 19, 'many': 36, 'machine': 35, 'method': 38, 'became': 6, 'widespread': 85, 'natural': 45, 'processing': 59, 'popularity': 57, 'due': 17, 'partly': 53, 'flurry': 20, 'result': 64, 'showing': 67, 'technique': 76, 'achieve': 0, 'state': 70, 'art': 4, 'task': 75, 'modeling': 42, 'parsing': 52, 'increasingly': 30, 'important': 27, 'medicine': 37, 'healthcare': 24, 'nlp': 48, 'help': 25, 'analyze': 2, 'note': 49, 'text': 78, 'electronic': 18, 'health': 23, 'record': 61, 'would': 87, 'otherwise': 50, 'inaccessible': 29, 'study': 73, 'seeking': 65, 'improve': 28, 'care': 10, 'protect': 60, 'patient': 54, 'privacy': 58}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9lblj-q1kVEk",
        "outputId": "50b82795-9644-4fe4-fe51-30b1724c22c9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'word n gram model time best statistical algorithm outperformed multi layer perceptron single hidden layer context length several word trained million word cpu cluster language modelling yoshua bengio co author'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature Array for First Sentence:\", x[0].toarray())   #Shows the word frequency array for the first sentence."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTgNFDVskHcC",
        "outputId": "d51804a5-5750-4810-9510-d10aaaf10d85"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Array for First Sentence: [[0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 2 0 1 0\n",
            "  0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            "  0 0 0 0 0 0 0 1 0 1 0 0 0 0 3 0 0 1]]\n"
          ]
        }
      ]
    }
  ]
}